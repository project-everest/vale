///////////////////////////////////////////////////////////////////////////////
//
// Based on poly1305-x86_64.pl from OpenSSL 1.1.1-dev
// See https://github.com/openssl/openssl/blob/master/crypto/poly1305/asm/poly1305-x86_64.pl
// The original file contains the following notices:
//
// # ==================================================================== 
// # Copyright 2016 The OpenSSL Project Authors. All Rights Reserved.
// #
// # Licensed under the OpenSSL license (the "License").  You may not use
// # this file except in compliance with the License.  You can obtain a copy
// # in the file LICENSE in the source distribution or at
// # https://www.openssl.org/source/license.html
// #
// # ====================================================================
// # Written by Andy Polyakov <appro@openssl.org> for the OpenSSL
// # project. The module is, however, dual licensed under OpenSSL and
// # CRYPTOGAMS licenses depending on where you obtain it. For further
// # details see http://www.openssl.org/~appro/cryptogams/.
// # ==================================================================== 
//
///////////////////////////////////////////////////////////////////////////////

include "../../../../arch/x64/X64.Vale.InsBasic.vaf"
include "../../../../arch/x64/X64.Vale.InsMem.vaf"

module X64.Poly1305

#reset-options "--z3rlimit 30"

#verbatim{:interface}{:implementation}
module M = Memory_i_s
open Types_s
open X64.Machine_s
open X64.Vale.State_i
open X64.Vale.Decls_i
open X64.Vale.InsBasic
open X64.Vale.InsMem
open X64.Vale.QuickCode_i
open X64.Vale.QuickCodes_i
//open X64.Vale.StrongPost_i
open Poly1305.Spec_s
open X64.Poly1305.Math_i
open X64.Poly1305.Util_i
open Opaque_i
#endverbatim

procedure{:quick} poly1305_multiply(ghost r1:nat64) returns(ghost hh:int)
    lets
        d1 @= r8; d2 @= r9; d3 @= r10; r0 @= r11; s1 @= r13; h0 @= r14; h1 @= rbx; h2 @= rbp;
        n := pow2_64;
        p := n * n * 4 - 5;
        r := r1 * n + r0;
        h := h2 * (n * n) + h1 * n + h0;
    reads
        r11; r13;
    modifies
        r8; r9; r10; r14; rbx; rbp;
        rax; rdx;
        efl;
    requires
        r1 % 4 == 0;
        s1 == r1 + r1 / 4;
        h2 * r0 < 7 * (n / 16);
        h0 * r1 < n * (n / 16);
        h1 * r0 < n * (n / 16);
        h2 * s1 < n * (n / 8);
        h0 * r0 < n * (n / 16);
        h1 * s1 < n * (n / 8);
        h2 * s1 < 7 * (5 * n / 64);
        rax == r1;
    ensures
        hh == (n * n) * d3 + n * h1 + h0;
        (h * r) % p == hh % p;
        d3 / 4 * 4 + d3 / 4 < 0x1_0000_0000_0000_0000;
        rax == 0xffff_ffff_ffff_fffc;
{
    assert h0 * r1 == r1 * h0;
    assert r0 * h0 == h0 * r0;
    assert r0 * h1 == h1 * r0;
    //assert r0 * h2 == h2 * r0;
    assert s1 * h1 == h1 * s1;
    //assert s1 * h2 == h2 * s1;

    ghost var gd0 := h0 * r0 + h1 * s1;
    ghost var gd1 := h0 * r1 + h1 * r0 + h2 * s1;
    ghost var gd2 := h2 * r0;
    // TODO: assert{:fast_block} true;
    assert va_is_src_opr64(va_op_opr64_reg(R14), this); // TODO: get rid of this
    Mul64Wrap(h0);  // h0*r1
    Mov64(d2, rax);
    Mov64(rax, r0);
    Mov64(d3, rdx);
    //assert n * d3 + d2 == old(h0 * r1);

    Mul64Wrap(h0);  // h0*r0
    Mov64(h0, rax); // future h0
    Mov64(rax, r0);
    Mov64(d1, rdx);
    //assert n * d1 + h0 == old(h0 * r0);

    Mul64Wrap(h1);  // h1*r0
    Add64Wrap(d2, rax);
    Mov64(rax, s1);
    Adc64Wrap(d3, rdx);
    //assert n * d3 + d2 == old(h0 * r1 + h1 * r0);

    Mul64Wrap(h1);  // h1*s1
    Mov64(h1, h2);  // borrow h1
    Add64Wrap(h0, rax);
    Adc64Wrap(d1, rdx);
    //assert n * d1 + h0 == old(h0 * r0 + h1 * s1);

    IMul64(h1, s1); // h2*s1
    Add64Wrap(d2, h1);
    Mov64(h1, d1);
    Adc64Wrap(d3, 0);
    //assert n * d3 + d2 == old(h0 * r1 + h1 * r0 + h2 * s1);

    IMul64(h2, r0); // h2*r0
    //assert h2 == gd2;
    Add64Wrap(h1, d2);
    Mov64(rax, 0xffff_ffff_ffff_fffc); // mask value
    Adc64Wrap(d3, h2);

    hh := (n * n) * d3 + n * h1 + h0;
    //assert hh == gd2 * (n * n) + gd1 * n + gd0;
    lemma_poly_multiply(n, p, r, h, r0, r1, old(h0), old(h1), old(h2), s1, gd0, gd1, gd2, hh);
}

procedure{:quick} poly1305_reduce() returns(ghost hh:int)
    lets
        d3 @= r10; h0 @= r14; h1 @= rbx; h2 @= rbp;
        n := 0x1_0000_0000_0000_0000;
        p := n * n * 4 - 5;
        hd := (n * n) * d3 + n * h1 + h0;
    modifies
        rax; d3; h0; h1; h2; efl;
    requires
        d3 / 4 * 4 + d3 / 4 < n;
        rax == 0xffff_ffff_ffff_fffc;
    ensures
        hh == (n * n) * h2 + n * h1 + h0;
        hd % p == hh % p;
        h2 < 5;
{
    lemma_poly_bits64();

    And64(rax, d3);
    Mov64(h2, d3);
    Shr64(d3, 2);
    And64(h2, 3);
    Add64Wrap(rax, d3);
    Add64Wrap(h0, rax);
    Adc64Wrap(h1, 0);
    Adc64Wrap(h2, 0);

    ghost var h10 := n * old(h1) + old(h0);
    hh := h10 + rax + (old(d3) % 4) * (n * n);
    lemma_poly_reduce(n, p, hd, old(d3), h10, rax, hh);
}

procedure{:quick} poly1305_iteration(ghost r1:nat64) returns(ghost hh:int)
    lets
        d1 @= r8; d2 @= r9; d3 @= r10; r0 @= r11; s1 @= r13; h0 @= r14; h1 @= rbx; h2 @= rbp;
        n:nat := 0x1_0000_0000_0000_0000;
        p:pos := n * n * 4 - 5;
        r := r1 * n + r0;
        h := h2 * (n * n) + h1 * n + h0;
    reads
        r0; s1;
    modifies
        rax; rdx; d1; d2; d3; h0; h1; h2; efl;
    requires
        r0 < n / 16;
        r1 < n / 16;
        r1 % 4 == 0;
        s1 == r1 + r1 / 4;
        h2 < 7;
        rax == r1;
    ensures
        hh == (n * n) * h2 + n * h1 + h0;
        modp(h * r) == modp(hh); 
        h2 < 5;
{
//    Previous version used a forall statement, which isn't yet supported for F*
//    forall x:nat, xb, y:nat, yb :| x < xb && y < yb :: x * y < xb * yb
//    {
//        lemma_mul_strict_upper_bound(x, xb, y, yb);
//    }
    // TODO:  This version leads Z3 off into the weeds and times out with F* (but not Dafny)
    //assume forall x:nat, xb, y:nat, yb { x*y, xb*yb } :: x < xb && y < yb ==> x * y < xb * yb;

    lemma_mul_strict_upper_bound(h2, 7, r0, n / 16);
    lemma_mul_strict_upper_bound(h0, n, r1, n / 16);
    lemma_mul_strict_upper_bound(h1, n, r0, n / 16);
    lemma_mul_strict_upper_bound(h2, n, s1, n / 8);
    lemma_mul_strict_upper_bound(h0, n, r0, n / 16);
    lemma_mul_strict_upper_bound(h1, n, s1, n / 8);
    lemma_mul_strict_upper_bound(h2, 7, s1, 5 * n / 64);

    (ghost var hd) := poly1305_multiply(r1);
    hh := poly1305_reduce();
    reveal_opaque(modp');
    assert hh == (n * n) * h2 + n * h1 + h0 /\ (h * r) % p == hh % p;
}

procedure{:quick} poly1305_blocks(
    ghost r:int,
    ghost h_in:int,
    ghost ctx_b:buffer64,
    ghost inp_b:buffer64)
    returns(ghost h:int)
    lets 
        ctx @= rdi; inp @= rsi; len @= rdx; padbit @= rcx; d1 @= r8; d2 @= r9; d3 @= r10;
        r0 @= r11; r1 @= r12; s1 @= r13; h0 @= r14; h1 @= rbx; h2 @= rbp;
        n:nat := pow2_64;
        p:pos := n * n * 4 - 5;
    reads
        ctx; padbit; r11; r13;
    modifies
        inp; len; d1; d2; d3; r0; r1; s1; h0; h1; h2; 
        rax; rdx; r15;
        efl;
        mem;
    requires
        // Len is measured in bytes
        len % 16 == 0; // REVIEW: may be stronger than necessary
        inp + len < pow2_64;
        buffers_disjoint(ctx_b, inp_b);
        validSrcAddrs64(mem, ctx, ctx_b, 24);
        validSrcAddrs64(mem, inp, inp_b, len / 8);
// TODO: Would be nice to have some syntax for buffer64_read
        let h0_in := buffer64_read(ctx_b, 0, mem);
        let h1_in := buffer64_read(ctx_b, 1, mem);
        let h2_in := buffer64_read(ctx_b, 2, mem);
        let r0_in := buffer64_read(ctx_b, 3, mem);
        let r1_in := buffer64_read(ctx_b, 4, mem);
        h_in == h2_in * (n * n) + h1_in * n + h0_in;
        r == r1_in * n + r0_in;
        r0_in < n / 16;
        r1_in < n / 16;
        r1_in % 4 == 0;
        h2_in < 5;
        padbit < 2;
    ensures
        h2 < 5;
        validSrcAddrs64(mem, ctx, ctx_b, 24);
        validSrcAddrs64(mem, old(inp), inp_b, old(len)/8);
        modifies_buffer_specific(ctx_b, old(mem), mem, 0, 2);
        h0 == buffer64_read(ctx_b, 0, mem);
        h1 == buffer64_read(ctx_b, 1, mem);
        h2 == buffer64_read(ctx_b, 2, mem);
        r0 == buffer64_read(ctx_b, 3, mem);
        r1 == buffer64_read(ctx_b, 4, mem);
        s1 == r1 + r1 / 4;
        inp == old(inp + len);

        // Framing
        rcx == old(rcx);
        ctx == old(ctx); // REVIEW: framing should add this automatically

        let r0_in := buffer64_read(ctx_b, 3, mem);
        let r1_in := buffer64_read(ctx_b, 4, mem);
        h == h2 * (pow2_64 * pow2_64) + h1 * pow2_64 + h0;
        modp(h) == poly1305_heap_blocks(modp(h_in), padbit * (n * n), r, buffer64_as_seq(mem, inp_b), old(len) / 8);
{
    lemma_poly_bits64();

    ghost var length:int := len;

    Shr64(len, 4);  // num_bytes / 16 ==> number of blocks
    // Slight difference: the original code has a special case for len == 0 here.
    // We can let len == 0 pass through because of the slight difference in the loop condition (see below)
    Mov64(r15, len); // reassign len

    Load64_buffer(r0, ctx, 24, ctx_b, 3); // load r
    Load64_buffer(s1, ctx, 32, ctx_b, 4);

    Load64_buffer(h0, ctx, 0, ctx_b, 0); // load hash value
    Load64_buffer(h1, ctx, 8, ctx_b, 1);
    Load64_buffer(h2, ctx, 16, ctx_b, 2);

    Mov64(r1, s1);
    Shr64(s1, 2);
    Mov64(rax, r1);
    Add64(s1, r1); // s1 = r1 + (r1 >> 2)

    h := h_in;
    assert modp(h) == poly1305_heap_blocks(modp(h_in), padbit * (n * n), r, buffer64_as_seq(mem, inp_b), 0) by 
    {
        reveal_opaque(modp');
        reveal_poly1305_heap_blocks(modp(h_in), padbit * (n * n), r, buffer64_as_seq(mem, inp_b), 0); 
    }

    ghost var word_index:int := 0;

    while (r15 != 0) // Slight difference: the original code uses the zero flag from "len-=16" rather than comparing len to 0
        invariant
            n == pow2_64; // REVIEW: not as good as "let n := pow2_64", because it doesn't let F* substitute pow2_64 for n
            n * n == pow2_64 * pow2_64; // REVIEW: see previous comment
            p == n * n * 4 - 5;
            r == r1 * n + r0;
            h == h2 * (pow2_64 * pow2_64) + h1 * pow2_64 + h0;
            r0 < n / 16;
            r1 < n / 16;
            r1 % 4 == 0;
            s1 == r1 + r1 / 4;
            h2 < 5;
            rax == r1;
            inp + 16 * r15 == old(inp) + length;
            old(inp) + length < pow2_64;
            length == old(len);

            r15 != 0 ==> 8*(word_index + 1) <= length;
            16 * r15 + 8 * word_index == length;
            inp + 0 /* offset */ == buffer_addr(inp_b) + 8 * word_index;

            r15 * 16 <= length;    // Not needed with Dafny version
            padbit < 2;            // Not needed with Dafny version 
            validSrcAddrs64(mem, ctx, ctx_b, 24);
            validSrcAddrs64(mem, old(inp), inp_b, length/8);
            ctx == old(ctx); // REVIEW: framing should add this automatically
            rcx == old(rcx); // REVIEW: framing should add this automatically
            (inp - old(inp)) % 16 == 0;             // Precondition for poly1305_heap_blocks; Not needed in Dafny version
            modp(h) == poly1305_heap_blocks(modp(h_in), padbit * (n * n), r, buffer64_as_seq(mem, inp_b), word_index);
            mem == old(mem);
        decreases
            r15;
    {
        let nn := pow2_64;
        ghost var hp := h;
        h := h + nn * nn * padbit + nn * buffer64_read(inp_b, word_index + 1, mem) + buffer64_read(inp_b, word_index, mem);
        ghost var hq := h;

        Add64Wrap(h0, Mem(inp, 0, inp_b, word_index)); // accumulate input
        lemma_load_mem64(inp_b, word_index, mem);   // TODO: Really shouldn't need this, but it's the only connection between eval_operand and buffer64_read
        Adc64Wrap(h1, Mem(inp, 8, inp_b, word_index+1));
        lemma_load_mem64(inp_b, word_index + 1, mem);   // TODO: Really shouldn't need this, but it's the only connection between eval_operand and buffer64_read
        AddLea64(inp, inp, 16);
        Adc64Wrap(h2, padbit);

        assert hq == h2 * (nn * nn) + h1 * nn + h0;

        h := poly1305_iteration(r1);

        Mov64(rax, r1);
        Sub64(r15, 1); // len-=16
        word_index := word_index + 2;

        assert modp(h) == poly1305_heap_blocks(modp(h_in), padbit * (nn * nn), r, buffer64_as_seq(mem, inp_b), word_index) by
        {
            reveal_poly1305_heap_blocks(modp(h_in), padbit * (nn * nn), r, buffer64_as_seq(mem, inp_b), word_index);
            reveal_poly1305_heap_blocks(modp(h_in), padbit * (nn * nn), r, buffer64_as_seq(mem, inp_b), word_index - 2);
            reveal_opaque(modp');
            lemma_poly_demod(p, hp, hq - hp, r);
        }
    }
    Store64_buffer(ctx, h0, 0, ctx_b, 0);
    Store64_buffer(ctx, h1, 8, ctx_b, 1);
    Store64_buffer(ctx, h2, 16, ctx_b, 2);
}

// last 1..15 bytes, in case len is not a multiple of 16
procedure{:quick} poly1305_last_block()
    lets
        h0 @= r14; h1 @= rbx; h2 @= rbp; r0 @= r11; s1 @= r13; nExtra @= r15;
        n:nat := 0x1_0000_0000_0000_0000;
        p:pos := n * n * 4 - 5;
        r1 := rax;
        r := lowerUpper128_opaque(r0, r1);
        hBlocks := lowerUpper192_opaque(lowerUpper128_opaque(h0, h1), h2);
        inpLast := lowerUpper128_opaque(r8, r9);
    reads
        r0; s1; nExtra;
    modifies
        rax; rcx; rdx; r8; r9; r10; h0; h1; h2; efl;
    requires
        h2 < 5;
        r0 < n / 16;
        r1 < n / 16;
        r1 % 4 == 0;
        s1 == r1 + r1 / 4;
        1 <= nExtra < 16;
    ensures
        h2 < 5;
        let padLast := pow2(nExtra * 8);
        let hLast := lowerUpper192_opaque(lowerUpper128_opaque(h0, h1), h2);
        modp(hLast) == modp((modp(hBlocks) + padLast + (inpLast % padLast)) * r);
{
    let padLast := pow2(nExtra * 8);

    if (nExtra < 8) {
        lemma_bytes_shift_power2(nExtra);
        Mov64(rcx, nExtra);
        Shl64(rcx, 3);
        Mov64(rdx, 1);
        Shl64(rdx, rcx);
        assert rdx == padLast;

        lemma_bytes_and_mod(r8, nExtra);
        //assert iand(r8, shift_left64(1, shift_left64(nExtra, 3)) - 1) == r8 % shift_left64(1, shift_left64(nExtra, 3));
        assert padLast == shift_left64(1, shift_left64(nExtra, 3));
        lemma_mod_power2_lo(r8, r9, nExtra, pow2(nExtra * 8));
        Mov64(rcx, rdx);
        Sub64(rcx, 1);
        And64(r8, rcx);
        Mov64(r9, 0);
        assert r8 == old(r8) % padLast;
        assert lowerUpper128_opaque(r8, r9) == inpLast % padLast;

        // h += (inpLast % padLast)
        Add64Wrap(h0, r8);
        Adc64Wrap(h1, r9);
        Adc64Wrap(h2, 0);

        Add64Wrap(h0, rdx);
        Adc64Wrap(h1, 0);
        Adc64Wrap(h2, 0);
    } else {
        lemma_bytes_shift_power2(nExtra - 8);
        Mov64(rcx, nExtra);
        Sub64(rcx, 8);
        Shl64(rcx, 3);
        Mov64(rdx, 1);
        Shl64(rdx, rcx);

        assert padLast == lowerUpper128_opaque(0, rdx) by {
            lemma_power2_add64(8 * nExtra - 64);
            reveal_opaque(lowerUpper128);
        }

        // inpLast := (inpLast % padLast)
        lemma_bytes_and_mod(r9, nExtra - 8);
        lemma_mod_hi(r8, r9, pow2(8 * (nExtra - 8)));
        Mov64(rcx, rdx);
        Sub64(rcx, 1);
        And64(r9, rcx);
        assert lowerUpper128_opaque(r8, r9) == inpLast % padLast;

        // h += (inpLast % padLast)
        Add64Wrap(h0, r8);
        Adc64Wrap(h1, r9);
        Adc64Wrap(h2, 0);

        Add64Wrap(h0, 0);
        Adc64Wrap(h1, rdx);
        Adc64Wrap(h2, 0);
    }

    ghost var h := hBlocks + (inpLast % padLast) + padLast;
    assert h == h2 * (n * n) + h1 * n + h0 by { reveal_opaque(lowerUpper192); reveal_opaque(lowerUpper128); }
    assert r == r1 * n + r0 by { reveal_opaque(lowerUpper128); }
    (ghost var hLast) := poly1305_iteration(r1);
    assert hLast == lowerUpper192_opaque(lowerUpper128_opaque(h0, h1), h2) by { reveal_opaque(lowerUpper192); reveal_opaque(lowerUpper128); }
    lemma_poly_demod(p, hBlocks, (inpLast % padLast) + padLast, r);
    assert modp(hLast) == modp((modp(hBlocks) + padLast + (inpLast % padLast)) * r) by { reveal_opaque(modp'); }
}

// h := (h % p) % 2^128;
procedure{:quick} poly1305_reduce_last()
    lets
        h0 @= r14; h1 @= rbx; h2 @= rbp;
        h := lowerUpper192_opaque(lowerUpper128_opaque(h0, h1), h2);
    modifies
        r8; r9; r10; rax; h0; h1; h2; efl;
    requires
        h2 < 5;
    ensures
        lowerUpper128_opaque(h0, h1) == mod2_128(modp(h));
{
    lemma_poly_bits64();

    Mov64(r8, h0);
    Mov64(r9, h1);
    Mov64(r10, h2);
    Add64Wrap(r8, 5);
    Adc64Wrap(r9, 0);
    Adc64Wrap(r10, 0);

    assert h + 5 == lowerUpper192_opaque(lowerUpper128_opaque(r8, r9), r10)
        by { reveal_opaque(lowerUpper128); reveal_opaque(lowerUpper192); }
    lemma_reduce128(h, old(h2), old(h1), old(h0), h + 5, r10, r9, r8);

    Shr64(r10, 2);

    Mov64(rax, r10);
    Sub64Wrap(rax, 1); // mask of ones if h < p, zero otherwise
    //assert rax == (if r10 == 0 then 0xffff_ffff_ffff_ffff else 0);
    And64(h0, rax);
    And64(h1, rax);

    Mov64(rax, 0);
    Sub64Wrap(rax, r10); // mask of ones if p <= h < 2 * p, zero otherwise
    //assert rax == (if r10 == 1 then 0xffff_ffff_ffff_ffff else 0);
    And64(r8, rax);
    And64(r9, rax);

    // Either h1 == h0 == 0 or r9 == r8 == 0; add to select the nonzero one:
    Add64(h0, r8);
    Add64(h1, r9);
}

// h := (h + key_s) % 2^128
procedure{:quick} poly1305_add_key_s()
    lets
        h0 @= r14; h1 @= rbx; key_s0 @= rax; key_s1 @= rdx;
        h_in := lowerUpper128_opaque(h0, h1);
        key_s := lowerUpper128_opaque(key_s0, key_s1);
    reads
        key_s0; key_s1;
    modifies
        h0; h1; efl;
    ensures
        lowerUpper128_opaque(h0, h1) == mod2_128(h_in + key_s);
{
    Add64Wrap(h0, key_s0);
    Adc64Wrap(h1, key_s1);

    lemma_add_key(old(h0), old(h1), h_in, key_s0, key_s1, key_s, h0, h1);
}

#verbatim
// REVIEW: not clear why TypesNative_s.reveal_iand doesn't work directly
let reveal_logand128 (x y:nat128) : Lemma
  (requires True)
  (ensures Types_s.iand x y == FStar.UInt.logand #128 x y)
  = TypesNative_s.reveal_iand 128 x y
#endverbatim

procedure{:quick} poly1305_impl(
    ghost key_r:nat128,
    ghost key_s:nat128,
    ghost ctx_b:buffer64,
    ghost inp_b:buffer64)
    returns(ghost h:int)
    lets
        ctx @= rdi; inp @= rsi; len @= rdx; r0 @= r11; r1 @= r12; h0 @= r14; h1 @= rbx; h2 @= rbp;
        n := pow2_64;
    modifies
        rax; rcx; rdx; r8; r9; r10; r13; r15;
        rdi; rsi; rdx; r11; r12; r14; rbx; rbp;
        efl;
        mem;
    requires
        buffers_disjoint(ctx_b, inp_b);
        validSrcAddrs64(mem, ctx, ctx_b, 24);
        validSrcAddrs64(mem, inp, inp_b, readable_words(len));
        inp + len < pow2_64;
        let key_r0 := buffer64_read(ctx_b, 3, mem);
        let key_r1 := buffer64_read(ctx_b, 4, mem);
        let key_s0 := buffer64_read(ctx_b, 5, mem);
        let key_s1 := buffer64_read(ctx_b, 6, mem);
        key_r == lowerUpper128_opaque(key_r0, key_r1);
        key_s == lowerUpper128_opaque(key_s0, key_s1);
    ensures
        validSrcAddrs64(mem, ctx, ctx_b, 24);
        validSrcAddrs64(mem, old(inp), inp_b, readable_words(old(len)));
        modifies_buffer(ctx_b, old(mem), mem);
        modifies_buffer_specific(ctx_b, old(mem), mem, 0, 8);
        h == lowerUpper128_opaque(h0, h1);
        let inp_mem := seqTo128(buffer64_as_seq(mem, inp_b));
        h == poly1305_hash(key_r, key_s, inp_mem, old(len));
//        // Framing
        ctx == old(ctx);
{
    ghost var inp_in := inp;
    ghost var len_in := len;
    ghost var key_r0 := buffer64_read(ctx_b, 3, mem);
    ghost var key_r1 := buffer64_read(ctx_b, 4, mem);
    lemma_poly_bits64();

    //assert{:fast_block} true;
    Mov64(rax, 0);
    Store64_buffer(ctx, rax,  0, ctx_b, 0);
    Store64_buffer(ctx, rax,  8, ctx_b, 1);
    Store64_buffer(ctx, rax, 16, ctx_b, 2);

    Load64_buffer(r0, ctx, 24, ctx_b, 3);
    Load64_buffer(r1, ctx, 32, ctx_b, 4);
    Mov64(rcx, 0x0fff_fffc_0fff_ffff);
    And64(r0, rcx);
    Mov64(rcx, 0x0fff_fffc_0fff_fffc);
    And64(r1, rcx);
    Store64_buffer(ctx, r0, 24, ctx_b, 3);
    Store64_buffer(ctx, r1, 32, ctx_b, 4);

    ghost var r:nat128 := lowerUpper128_opaque(r0, r1);
    assert r == r0 + n * r1 by { reveal_opaque(lowerUpper128); }

    let mask:nat128 := 0x0ffffffc_0ffffffc_0ffffffc_0fffffff; // REVIEW: why do we need to put this constant in a variable?
    assert r == iand(key_r, mask) by
    {
        reveal_opaque(lowerUpper128);
        lemma_lowerUpper128_and(key_r, key_r0, key_r1, mask,
            0x0fff_fffc_0fff_ffff, 0x0fff_fffc_0fff_fffc, r, r0, r1);
    }

    Mov64(rax, len);
    And64(rax, 15);
    Sub64(len, rax);
    // assert rax == len_in % 16;
    // assert len == len_in / 16 * 16; == (num 16-byte blocks) * 16
    Store64_buffer(ctx, rax, 56, ctx_b, 7);
    Store64_buffer(ctx, len, 64, ctx_b, 8);

    Mov64(rcx, 1);
    h := poly1305_blocks(r, 0, ctx_b, inp_b);
    assert modp(0) == 0 by
    {
        modp_0();
//        reveal_opaque(modp');
//        assert_by_tactic(modp(0) == 0, retain_only_modp());
    }
    lemma_poly1305_heap_hash_blocks_alt(0, n * n, r, mem, inp_b, old(len) / 16);

    reveal_logand128(key_r, mask);
    assert r == bare_r(key_r);
    assert h == lowerUpper192_opaque(lowerUpper128_opaque(h0, h1), h2)
        by { reveal_opaque(lowerUpper192); reveal_opaque(lowerUpper128); }

    Load64_buffer(r15, ctx, 56, ctx_b, 7); 
    // assert r15 == len_in % 16;
    if (r15 != 0)
    {
        //assert{:fast_block} true;
        Load64_buffer(rax, ctx, 32, ctx_b, 4);
        Load64_buffer(r8, inp, 0, inp_b, (len_in / 16) * 2);
        Load64_buffer(r9, inp, 8, inp_b, (len_in / 16) * 2 + 1);
        ghost var a := seqTo128(buffer64_as_seq(mem, inp_b), len_in / 16);
        assert lowerUpper128_opaque(r8, r9) == a
            by { /* reveal heapletTo128; */ reveal_opaque(lowerUpper128); }
        poly1305_last_block();
        h := lowerUpper192_opaque(lowerUpper128_opaque(h0, h1), h2);
    }

    lemma_add_mod128(modp(h), key_s);
    poly1305_reduce_last();
    h := lowerUpper128_opaque(h0, h1);

    Load64_buffer(rax, ctx, 40, ctx_b, 5);
    Load64_buffer(rdx, ctx, 48, ctx_b, 6);
    poly1305_add_key_s();
    h := lowerUpper128_opaque(h0, h1);

    assert h == poly1305_hash(key_r, key_s, seqTo128(buffer64_as_seq(mem, inp_b)), len_in)
        by { reveal_opaque(mod2_128'); reveal_opaque(modp'); }
}

// poly1305(ctx, inp, len)
//
// Note that this reads 16-byte chunks directly from the input buffer,
// so (len + 15) / 16 * 16 bytes must be readable, even though only len bytes
// affect the result.
procedure{:quick} poly1305(
    inline win:bool,
    ghost key_r:nat128,
    ghost key_s:nat128,
    ghost ctx_b:buffer64,
    ghost inp_b:buffer64)
    returns(ghost h:int)
    lets
        ctx @= rdi; inp @= rsi; len @= rdx; r0 @= r11; r1 @= r12; h0 @= r14; h1 @= rbx; h2 @= rbp;
        ctx_in := (if win then rcx else ctx);
        inp_in := (if win then rdx else inp);
        len_in := (if win then r8 else len);
        n := 0x1_0000_0000_0000_0000;
        p := n * n * 4 - 5;
    modifies
        rax; rcx; rdx; r8; r9; r10; r13; r15; ctx; inp; len; r0; r1; h0; h1; h2; efl; mem;
    requires
        buffers_disjoint(ctx_b, inp_b);
        validSrcAddrs64(mem, ctx_in, ctx_b, 24);
        validSrcAddrs64(mem, inp_in, inp_b, readable_words(len_in));
        inp_in + len_in < pow2_64;
        let key_r0 := buffer64_read(ctx_b, 3, mem);
        let key_r1 := buffer64_read(ctx_b, 4, mem);
        let key_s0 := buffer64_read(ctx_b, 5, mem);
        let key_s1 := buffer64_read(ctx_b, 6, mem);
        key_r == lowerUpper128_opaque(key_r0, key_r1);
        key_s == lowerUpper128_opaque(key_s0, key_s1);
    ensures
        validSrcAddrs64(mem, ctx_in, ctx_b, 24);
        validSrcAddrs64(mem, inp_in, inp_b, readable_words(len_in));
        modifies_buffer(ctx_b, old(mem), mem);
        let h0_out := buffer64_read(ctx_b, 0, mem);
        let h1_out := buffer64_read(ctx_b, 1, mem);
        h == lowerUpper128_opaque(h0_out, h1_out);
        let inp_mem := seqTo128(buffer64_as_seq(mem, inp_b));
        h == poly1305_hash(key_r, key_s, inp_mem, len_in);
        h1 == old(h1);
        h2 == old(h2);
        ctx == old(ctx);
        inp == old(inp);
        r1 == old(r1);
        r13 == old(r13);
        h0 == old(h0);
        r15 == old(r15);
{
    h := 0; 
    Mov64(rax, ctx);
    Mov64(r9, inp);
    inline if (win)
    {
        Mov64(ctx, rcx);
        Mov64(inp, rdx);
        Mov64(len, r8);
    }
    // assert ctx == ctx_in;
    // assert inp == inp_in;
    // assert len == len_in;

    // context:
    //   0, 8, 16: will hold h
    //   24, 32: key_r
    //   40, 48: key_s
    //   56: will hold len % 16
    //   64: will hold len / 16 * 16
    //   72, 80, 88, 96, 104, 112, 120, 128: callee-save registers
    //assert {:fast_block} true;
    Store64_buffer(ctx, h1,  72, ctx_b, 9);
    Store64_buffer(ctx, h2,  80, ctx_b, 10);
    Store64_buffer(ctx, rax, 88, ctx_b, 11);
    Store64_buffer(ctx, r9,  96, ctx_b, 12);
    Store64_buffer(ctx, r1,  104, ctx_b, 13);
    Store64_buffer(ctx, r13, 112, ctx_b, 14);
    Store64_buffer(ctx, h0,  120, ctx_b, 15);
    Store64_buffer(ctx, r15, 128, ctx_b, 16);

    h := poly1305_impl(key_r, key_s, ctx_b, inp_b);

    //assert {:fast_block} true;
    Store64_buffer(ctx, h0,  0, ctx_b, 0);
    Store64_buffer(ctx, h1,  8, ctx_b, 1);

    Load64_buffer(h1,  ctx, 72, ctx_b, 9);
    Load64_buffer(h2,  ctx, 80, ctx_b, 10);
    Load64_buffer(rax, ctx, 88, ctx_b, 11);
    Load64_buffer(inp, ctx, 96, ctx_b, 12);
    Load64_buffer(r1,  ctx, 104, ctx_b, 13);
    Load64_buffer(r13, ctx, 112, ctx_b, 14);
    Load64_buffer(h0,  ctx, 120, ctx_b, 15);
    Load64_buffer(r15, ctx, 128, ctx_b, 16);
    Mov64(ctx, rax);
}
