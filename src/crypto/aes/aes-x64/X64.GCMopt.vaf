include "../../../arch/x64/X64.Vale.InsBasic.vaf"
include "../../../arch/x64/X64.Vale.InsMem.vaf"
include "../../../arch/x64/X64.Vale.InsVector.vaf"
include "X64.AES.vaf"
include "X64.GF128_Mul.vaf"

//include "X64.GCTR.vaf"
//include "X64.GHash.vaf"

module X64.GCMopt

#verbatim{:interface}{:implementation}
open FStar.Seq
open Words_s
open Types_s
open Types_i
open AES_s
open GCTR_s
open GCTR_i
open GHash_s
open GHash_i
open GCM_s
open X64.AES
open GF128_s
open GF128_i
//open X64.GHash
//open X64.GCTR
open X64.Machine_s
open X64.Vale.State_i
open X64.Vale.Decls_i
open X64.Vale.InsBasic
open X64.Vale.InsMem
open X64.Vale.InsVector
open X64.Vale.InsAes
open X64.Vale.QuickCode_i
open X64.Vale.QuickCodes_i
open X64.GF128_Mul
#endverbatim

#verbatim{:interface}
//let get_last_slice_workaround (s:seq quad32) (start_pos end_pos:int)  =
//  if 0 <= start_pos && start_pos < end_pos && end_pos <= length s then
//    last (slice s start_pos end_pos)
//  else
//    Mkfour 0 0 0 0 
//
//let slice_workaround (s:seq quad32) (start_pos end_pos:int)  =
//  if 0 <= start_pos && start_pos < end_pos && end_pos <= length s then
//    slice s start_pos end_pos
//  else
//    create 1 (Mkfour 0 0 0 0)
#endverbatim


///////////////////////////
//  Useful helper steps
///////////////////////////

procedure {:quick exportOnly} zero_xmm(inout dst:xmm)
    modifies efl;
    ensures
        dst == Mkfour(0, 0, 0, 0);
{
    Pxor(dst, dst);
    lemma_quad32_xor();
}

procedure {:quick exportOnly} init_pshufb_mask(inout dst:xmm)
    modifies efl;
    ensures
        dst == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
{
    // TODO: Add an instruction to simplify this
    Pinsrd(dst, 0x0C0D0E0F, 0);
    Pinsrd(dst, 0x08090A0B, 1);
    Pinsrd(dst, 0x04050607, 2);
    Pinsrd(dst, 0x00010203, 3);
}

procedure {:quick exportOnly} inc32(inout dst:xmm, one:xmm)
    requires
        one == Mkfour(1, 0, 0, 0);
    modifies 
        efl;
    ensures 
        dst == inc32(old(dst), 1);
{
    Paddd(dst, one);
}

// GCTR encrypt one block
procedure {:quick} gctr_register(
    ghost key:aes_key(AES_128),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    lets io @= xmm1; icb @= xmm7;
    reads r8; icb; mem;
    modifies
        xmm0; xmm1; xmm2; efl;

    requires
        // AES reqs
        length(round_keys) == 11;
        round_keys == key_to_round_keys(AES_128, reverse_bytes_nat32_seq(key));
        r8 == buffer_addr(keys_b);
        validSrcAddrs128(mem, r8, keys_b, 11);
        buffer128_as_seq(mem, keys_b) == round_keys;
        // GCM reqs
        xmm2 == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
    ensures
        io == quad32_xor(old(io), aes_encrypt_le(AES_128, key, icb));
{
    Mov128(xmm0, icb);
    Pshufb(xmm0, xmm2);
    AES128EncryptBlock(reverse_bytes_quad32(icb), reverse_bytes_nat32_seq(key), round_keys, keys_b);

    Pxor(xmm1, xmm0);

//    // Call a helpful lemma
//    gctr_encrypt_one_block(icb, old(io), AES_128, key);
}

// Multiply one block
procedure {:quick} compute_ghash_incremental()
    lets input @= xmm2; io @= xmm1; h @= xmm11;
    reads
        input; h;
    modifies
        io; efl;
        xmm2; xmm3; xmm4; xmm5; xmm6; 
    ensures
        io == ghash_incremental(h, old(io), create(1, old(input)));
{
    Pxor(io, input);    // Y_i := Y_{i-1} ^ x_i
    Mov128(xmm2, h);    // Move h into the register that ReduceMulRev128 expects

    lemma_to_of_quad32(io);   // Help satisfy precondition
    lemma_to_of_quad32(h);    // Help satisfy precondition
    ReduceMulRev128(gf128_of_quad32(io), gf128_of_quad32(h));    // io := Y_i * H
}


#reset-options "--z3rlimit 40"

///////////////////////////
// GCM
///////////////////////////
procedure {:quick} gcm_core(
    ghost in_b:buffer128,
    ghost out_b:buffer128,
    ghost key:aes_key(AES_128),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    lets in_ptr @= rax; out_ptr @= rbx; len @= rcx; icb @= xmm7; mask @= xmm8; hash @= xmm1; one @= xmm10; h @= xmm11;

    reads
        r8; in_ptr; out_ptr; len; h;

    modifies
        rdx; r9; r10; xmm0; xmm1; xmm2; xmm3; xmm4; xmm5; xmm6; icb; mask; hash; one; mem; efl;

    requires
        // GCTR reqs
        buffers_disjoint128(in_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        validSrcAddrs128(mem, in_ptr, in_b, len);
        validDstAddrs128(mem, out_ptr, out_b, len);
        in_ptr  + 16 * len < pow2_64;
        out_ptr + 16 * len < pow2_64;
        buffer_length(in_b) == buffer_length(out_b) /\ buffer_length(out_b) == len /\ 256 * buffer_length(in_b) < pow2_32;

        // AES reqs
        length(round_keys) == 11;
        round_keys == key_to_round_keys(AES_128, reverse_bytes_nat32_seq(key));
        r8 == buffer_addr(keys_b);
        validSrcAddrs128(mem, r8, keys_b, 11);
        buffer128_as_seq(mem, keys_b) == round_keys;
    ensures
        modifies_buffer128(out_b, old(mem), mem);
        validSrcAddrs128(mem, out_ptr, out_b, len);

        // GCTR
        buffer128_as_seq(mem, out_b) == gctr_encrypt(old(icb), buffer128_as_seq(old(mem), in_b), AES_128, key);
        icb.lo1 == old(icb.lo1);
        icb.hi2 == old(icb.hi2);
        icb.hi3 == old(icb.hi3);

        // GHash
        len == 0 ==> rdx == old(rdx) /\ r9 == old(r9) /\ hash == old(hash); 
        len > 0 ==> length(buffer128_as_seq(mem, out_b)) > 0 /\ hash == ghash_incremental(h, old(hash), buffer128_as_seq(mem, out_b));
{
    Mov64(rdx, 0);
    Mov64(r9, in_ptr);
    Mov64(r10, out_ptr);

    // Initialize counter
    zero_xmm(one);
    Pinsrd(one, 1, 0);

    init_pshufb_mask(mask);

    while (rdx != len)
        invariant
            //////////////////// Basic indexing //////////////////////
            0 <= rdx <= len;
            r9 == in_ptr + 16 * rdx;
            r10 == out_ptr + 16 * rdx;
            icb == inc32(old(icb), rdx);
            
            //////////////////// From requires //////////////////////
            // GCTR reqs
            buffers_disjoint128(in_b, out_b);
            buffers_disjoint128(keys_b, out_b);
            validSrcAddrs128(mem, in_ptr, in_b, len);
            validDstAddrs128(mem, out_ptr, out_b, len);
            in_ptr  + 16 * len < pow2_64;
            out_ptr + 16 * len < pow2_64;
            buffer_length(in_b) == buffer_length(out_b) /\ buffer_length(out_b) == len; 

            // AES reqs
            length(round_keys) == 11;
            round_keys == key_to_round_keys(AES_128, reverse_bytes_nat32_seq(key));
            r8 == buffer_addr(keys_b);
            validSrcAddrs128(mem, r8, keys_b, 11);
            buffer128_as_seq(mem, keys_b) == round_keys;

            //////////////////// GCTR invariants //////////////////////
            mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
            one == Mkfour(1, 0, 0, 0);

            //////////////////// Postcondition goals //////////////////////
            modifies_buffer128(out_b, old(mem), mem);
            validSrcAddrs128(mem, out_ptr, out_b, len);
            gctr_partial(rdx, buffer128_as_seq(mem, in_b), buffer128_as_seq(mem, out_b), key, old(icb));

            rdx == 0 ==> hash == old(hash);
            rdx > 0 ==> hash == ghash_incremental(h, old(hash), slice_workaround(buffer128_as_seq(mem, out_b), 0, rdx));

//            forall j :: 0 <= j < rdx ==> 
//                        buffer128_read(out_b, j, mem) == 
//                        quad32_xor(index_workaround(buffer128_as_seq(mem, in_b), j), aes_encrypt(AES_128, key, inc32(old(icb), j)));

        decreases
            len - rdx;
    {
        Mov128(xmm0, icb);
        Pshufb(xmm0, mask);
        AES128EncryptBlock(reverse_bytes_quad32(icb), reverse_bytes_nat32_seq(key), round_keys, keys_b);

        Load128_buffer(xmm2, r9, 0, in_b, rdx);
        Pxor(xmm2, xmm0);
        Store128_buffer(r10, xmm2, 0, out_b, rdx);

        // Update our hash
        compute_ghash_incremental();

        Add64(rdx, 1);
        Add64(r9, 16);
        Add64(r10, 16);
        inc32(icb, one);
    }
    
    // Call a helpful lemma
    gctr_partial_completed(buffer128_as_seq(mem, in_b), buffer128_as_seq(mem, out_b), key, old(icb));
}

/*
procedure {:quick} gcm_core(
    ghost plain_b:buffer128,
    ghost auth_b:buffer128,
    ghost out_b:buffer128,
    ghost key:aes_key(AES_128),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )

    lets plain_ptr @= r12; out_ptr @= rbx; keys_ptr @= r8; auth_ptr @= r14;
    plain_len @= r13; auth_len @= r11; 
    iv @= xmm7;

    reads
        plain_ptr; out_ptr; keys_ptr; auth_ptr; plain_len; auth_len; 

    modifies
        rax; rcx; rdx; r9; r10; xmm0; xmm1; xmm2; xmm3; xmm4; xmm5; iv;
        xmm6; 
        mem; efl; 


    requires
        // GCM reqs
        buffers_disjoint128(plain_b, out_b);
        buffers_disjoint128(auth_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        validSrcAddrs128(mem, plain_ptr, plain_b, plain_len);
        validSrcAddrs128(mem, auth_ptr, auth_b, auth_len);
        validDstAddrs128(mem, out_ptr, out_b, plain_len);
        plain_ptr + 16 * plain_len < pow2_64;
        auth_ptr + 16 * auth_len < pow2_64;
        out_ptr + 16 * plain_len < pow2_64;
        buffer_length(plain_b) == buffer_length(out_b) /\ buffer_length(out_b) == plain_len /\ 256 * buffer_length(plain_b) < pow2_32;
        buffer_length(auth_b) == auth_len;

        // To simplify length calculations, restrict auth and plain length further
        256 * auth_len < pow2_32;
        256 * plain_len < pow2_32;

        // AES reqs
        length(round_keys) == 11;
        round_keys == key_to_round_keys(AES_128, reverse_bytes_nat32_seq(key));
        validSrcAddrs128(mem, keys_ptr, keys_b, 11);
        buffer128_as_seq(mem, keys_b) == round_keys;
    ensures
        modifies_buffer128(out_b, old(mem), mem);
        validSrcAddrs128(mem, out_ptr, out_b, plain_len);
        256 * buffer_length(plain_b) < pow2_32;
        256 * buffer_length(auth_b) < pow2_32;
        buffer128_as_seq(mem, out_b) == fst(gcm_encrypt(AES_128, key, old(iv), buffer128_as_seq(old(mem), plain_b), buffer128_as_seq(old(mem), auth_b)));
        create(1, xmm1) == snd(gcm_encrypt(AES_128, key, old(iv), buffer128_as_seq(old(mem), plain_b), buffer128_as_seq(old(mem), auth_b)));
{
    // let h = aes_encrypt alg key (Mkfour 0 0 0 0) in
    Pxor(xmm0, xmm0);
    lemma_quad32_xor(); // xmm0 = 0
    AES128EncryptBlock(xmm0, reverse_bytes_nat32_seq(key), round_keys, keys_b); // h = xmm0 = aes_encrypt alg (reverse_bytes_nat32_seq key) (Mkfour 0 0 0 0) in
    ghost var h := xmm0;
    Mov128(xmm5, xmm0);     // Save a copy of h

    // let j0 = Mkfour 1 iv.mid_lo iv.mid_hi iv.hi in
    // (inc32 j0 1)
    Pinsrd(iv, 2, 0);
    // assert iv == inc32(old(Mkfour(1,iv.mid_lo,iv.mid_hi,iv.hi)), 1);

    // let c = gctr_encrypt (inc32 j0 1) p alg key in
    Mov64(rax, plain_ptr);
    Mov64(rcx, plain_len);
    gctr_core(plain_b, out_b, key, round_keys, keys_b);

    // Compute the hashes incrementally
    Mov64(rax, auth_ptr);
    Mov64(rcx, auth_len);
    Mov128(xmm0, xmm5);     // Move h where ghash expects it
    // assert xmm0 == h;
    ghost var y_0 := Mkfour(0, 0, 0, 0);
    compute_Y0();
    // assert xmm1 == y_0;
    compute_ghash_incremental(auth_b);
    ghost var y_auth := xmm1;

    Mov64(rax, out_ptr);
    Mov64(rcx, plain_len);

    // assert xmm0 == h;
    compute_ghash_incremental(out_b);
    ghost var y_cipher := xmm1;

    // Prepare length fields
    Pxor(xmm2, xmm2);
    Pinsrd(xmm2, auth_len, 3);
    Pinsrd(xmm2, plain_len, 1);

    ghost var length_quad32 := xmm2;

    compute_ghash_incremental_register();
    ghost var y_final := xmm1;

    // Invoke lemma showing that incremental hashing works
    lemma_hash_append3(h, y_0, y_auth, y_cipher, y_final,
                       buffer128_as_seq(mem, auth_b),
                       buffer128_as_seq(mem, out_b),
                       create(1, length_quad32)); 
    // assert xmm1 == ghash(h, append(buffer128_as_seq(mem, auth_b), append(buffer128_as_seq(mem, out_b), create(1, length_quad32)))); 
    ghost var hash := xmm1;

    Pinsrd(iv, 1, 0);   // Reconstruct j0 (this is all we need, since gctr_core says it only changes iv.lo)
    // assert iv == old(Mkfour(1,iv.mid_lo,iv.mid_hi,iv.hi));

    // Encrypt the hash value with gctr_register 
    gctr_register(key, round_keys, keys_b); // Encrypt using j0 and xmm0 = hash_value 
    // assert create(1, xmm1) == gctr_encrypt(old(Mkfour(1,iv.mid_lo,iv.mid_hi,iv.hi)), create(1, hash), AES_128, key);
}
*/


/*
procedure {:quick} callee_save_registers(ghost buffer:buffer64)
    lets ptr @= rax;
    requires 
        validDstAddrs64(mem, ptr, buffer, 8);
    reads rax; rbx; rbp; rdi; rsi; r12; r13; r14; r15; 
    modifies mem;
    ensures
        validDstAddrs64(mem, ptr, buffer, 8);
        modifies_buffer(buffer, old(mem), mem);

        rbx == buffer64_read(buffer, 0, mem);
        rbp == buffer64_read(buffer, 1, mem);
        rdi == buffer64_read(buffer, 2, mem);
        rsi == buffer64_read(buffer, 3, mem);
        r12 == buffer64_read(buffer, 4, mem);
        r13 == buffer64_read(buffer, 5, mem);
        r14 == buffer64_read(buffer, 6, mem);
        r15 == buffer64_read(buffer, 7, mem);
{
    Store64_buffer(ptr, rbx,  0, buffer, 0);
    Store64_buffer(ptr, rbp,  8, buffer, 1);
    Store64_buffer(ptr, rdi, 16, buffer, 2);
    Store64_buffer(ptr, rsi, 24, buffer, 3);
    Store64_buffer(ptr, r12, 32, buffer, 4);
    Store64_buffer(ptr, r13, 40, buffer, 5);
    Store64_buffer(ptr, r14, 48, buffer, 6);
    Store64_buffer(ptr, r15, 56, buffer, 7);
}

procedure {:quick} callee_restore_registers(ghost buffer:buffer64)
    lets ptr @= rax;
    requires 
        validSrcAddrs64(mem, ptr, buffer, 8);
    reads rax; mem; 
    modifies rbx; rbp; rdi; rsi; r12; r13; r14; r15; 
    ensures
        rbx == buffer64_read(buffer, 0, mem);
        rbp == buffer64_read(buffer, 1, mem);
        rdi == buffer64_read(buffer, 2, mem);
        rsi == buffer64_read(buffer, 3, mem);
        r12 == buffer64_read(buffer, 4, mem);
        r13 == buffer64_read(buffer, 5, mem);
        r14 == buffer64_read(buffer, 6, mem);
        r15 == buffer64_read(buffer, 7, mem);
{
    Load64_buffer(rbx, ptr,  0, buffer, 0);
    Load64_buffer(rbp, ptr,  8, buffer, 1);
    Load64_buffer(rdi, ptr, 16, buffer, 2);
    Load64_buffer(rsi, ptr, 24, buffer, 3);
    Load64_buffer(r12, ptr, 32, buffer, 4);
    Load64_buffer(r13, ptr, 40, buffer, 5);
    Load64_buffer(r14, ptr, 48, buffer, 6);
    Load64_buffer(r15, ptr, 56, buffer, 7);
}

#verbatim{:interface}
let mk_gctr_plain (p:seq quad32) : gctr_plain =
  let open FStar.Mul in
  if 256 * length p < pow2_32 then p else createEmpty
#endverbatim


procedure {:quick} gcm_stdcall_inner(
    ghost args_b:buffer64,
    ghost iv:quad32,

    ghost plain_b:buffer128,
    ghost auth_b:buffer128,
    ghost iv_b:buffer128,
    ghost out_b:buffer128,
    ghost tag_b:buffer128,

    ghost key:aes_key(AES_128),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    modifies
        rax; rbx; rcx; rdx; r8; r9; r10; r11; r12; r13; r14; r15; xmm0; xmm1; xmm2; xmm3; xmm4; xmm5; xmm6; xmm7;
        mem; efl;
    lets
        args_ptr := r9;

        plain_ptr         := buffer64_read(args_b, 0, mem);
        plain_len         := buffer64_read(args_b, 1, mem);
        auth_ptr          := buffer64_read(args_b, 2, mem);
        auth_len          := buffer64_read(args_b, 3, mem);
        iv_ptr            := buffer64_read(args_b, 4, mem);
        expanded_key_ptr  := buffer64_read(args_b, 5, mem);
        out_ptr           := buffer64_read(args_b, 6, mem);
        tag_ptr           := buffer64_read(args_b, 7, mem);

    requires
        validSrcAddrs64(mem, args_ptr, args_b, 9);

        buffers_disjoint128(plain_b, out_b);
        buffers_disjoint128(auth_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        buffers_disjoint128(out_b, tag_b);
        
        validSrcAddrs128(mem, plain_ptr, plain_b, plain_len);
        validSrcAddrs128(mem, auth_ptr, auth_b, auth_len);
        validSrcAddrs128(mem, iv_ptr, iv_b, 1);
        validDstAddrs128(mem, out_ptr, out_b, plain_len);
        validDstAddrs128(mem, tag_ptr, tag_b, 1);

        plain_ptr + 16 * plain_len < pow2_64;
        auth_ptr + 16 * auth_len < pow2_64;
        out_ptr + 16 * plain_len < pow2_64;
        buffer_length(plain_b) == buffer_length(out_b) /\ buffer_length(out_b) == plain_len /\ 256 * buffer_length(plain_b) < pow2_32;
        buffer_length(auth_b) == auth_len;

        iv == buffer128_read(iv_b, 0, mem);

        // To simplify length calculations, restrict auth and plain length further
        256 * auth_len < pow2_32;
        256 * plain_len < pow2_32;

        // AES reqs
        length(round_keys) == 11;
        round_keys == key_to_round_keys(AES_128, reverse_bytes_nat32_seq(key));
        validSrcAddrs128(mem, expanded_key_ptr, keys_b, 11);
        buffer128_as_seq(mem, keys_b) == round_keys;

    ensures
        modifies_buffer128_2(out_b, tag_b, old(mem), mem);

        validSrcAddrs128(mem, old(out_ptr), out_b, plain_len);
        validSrcAddrs128(mem, old(tag_ptr), tag_b, 1);

        256 * buffer_length(plain_b) < pow2_32;
        256 * buffer_length(auth_b) < pow2_32;
        buffer128_as_seq(mem, out_b) == fst(gcm_encrypt(AES_128, key, iv, buffer128_as_seq(old(mem), plain_b), buffer128_as_seq(old(mem), auth_b)));
        create(1, buffer128_read(tag_b, 0, mem)) == snd(gcm_encrypt(AES_128, key, iv, mk_gctr_plain(buffer128_as_seq(old(mem), plain_b)), mk_gctr_plain(buffer128_as_seq(old(mem), auth_b))));
{
    Load64_buffer(r12, r9,  0, args_b, 0);
    //assert r12 == plain_ptr;
    Load64_buffer(r13, r9,  8, args_b, 1);
    //assert r13 == plain_len;
    Load64_buffer(r14, r9, 16, args_b, 2);
    Load64_buffer(r11, r9, 24, args_b, 3);
    Load64_buffer(r10, r9, 32, args_b, 4);
    Load64_buffer(r8,  r9, 40, args_b, 5);
    Load64_buffer(rbx, r9, 48, args_b, 6);
    Load64_buffer(r15, r9, 56, args_b, 7);

    // Load the IV into its XMM register
    Load128_buffer(xmm7, r10, 0, iv_b, 0);

    //assert iv == xmm7;
//    assert {:quick_end} true;

    gcm_core(plain_b, auth_b, out_b, key, round_keys, keys_b);

//    assert buffer128_as_seq(mem, out_b) == fst(gcm_encrypt(AES_128, key, iv, mk_gctr_plain(buffer128_as_seq(old(mem), plain_b)), mk_gctr_plain(buffer128_as_seq(old(mem), auth_b))));

    //assert create(1, xmm1) == snd(gcm_encrypt(AES_128, key, iv, mk_gctr_plain(buffer128_as_seq(old(mem), plain_b)), mk_gctr_plain(buffer128_as_seq(old(mem), auth_b))));
    // Auth tag is still in xmm1, so save it to memory
    Store128_buffer(r15, xmm1, 0, tag_b, 0);
    //assert buffer128_as_seq(mem, tag_b) == create(1, xmm1);
    //assert buffer128_read(tag_b, 0, mem) == create(1, xmm1);

    //assert create(1, buffer128_read(tag_b, 0, mem)) == snd(gcm_encrypt(AES_128, key, iv, mk_gctr_plain(buffer128_as_seq(old(mem), plain_b)), mk_gctr_plain(buffer128_as_seq(old(mem), auth_b))));
    //assert buffer128_as_seq(mem, tag_b) == snd(gcm_encrypt(AES_128, key, iv, mk_gctr_plain(buffer128_as_seq(old(mem), plain_b)), mk_gctr_plain(buffer128_as_seq(old(mem), auth_b))));
}

procedure {:quick} gcm_stdcall(
    inline win:bool,
    ghost args_b:buffer64,
    ghost iv:quad32,

    ghost plain_b:buffer128,
    ghost auth_b:buffer128,
    ghost iv_b:buffer128,
    ghost out_b:buffer128,
    ghost tag_b:buffer128,
    ghost tmp_b:buffer64,

    ghost key:aes_key(AES_128),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    modifies
        rax; rbx; rcx; rdx; r8; r9; r10; r11; r12; r13; r14; r15; xmm0; xmm1; xmm2; xmm3; xmm4; xmm5; xmm6; xmm7;
        rbp; rdi; rsi; 
        mem; efl;
    lets
        args_ptr := if win then rcx else rdi;

        plain_ptr         := buffer64_read(args_b, 0, mem);
        plain_len         := buffer64_read(args_b, 1, mem);
        auth_ptr          := buffer64_read(args_b, 2, mem);
        auth_len          := buffer64_read(args_b, 3, mem);
        iv_ptr            := buffer64_read(args_b, 4, mem);
        expanded_key_ptr  := buffer64_read(args_b, 5, mem);
        out_ptr           := buffer64_read(args_b, 6, mem);
        tag_ptr           := buffer64_read(args_b, 7, mem);
        tmp_ptr           := buffer64_read(args_b, 8, mem);

    requires
        validSrcAddrs64(mem, args_ptr, args_b, 9);

        buffers_disjoint128(plain_b, out_b);
        buffers_disjoint128(auth_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        buffers_disjoint128(out_b, tag_b);
        
        buffers_disjoint(args_b, tmp_b);

        locs_disjoint(list(loc_buffer(tmp_b), loc_buffer(plain_b)));
        locs_disjoint(list(loc_buffer(tmp_b), loc_buffer(auth_b)));
        locs_disjoint(list(loc_buffer(tmp_b), loc_buffer(iv_b)));
        locs_disjoint(list(loc_buffer(tmp_b), loc_buffer(out_b)));
        locs_disjoint(list(loc_buffer(tmp_b), loc_buffer(tag_b)));
        locs_disjoint(list(loc_buffer(tmp_b), loc_buffer(keys_b)));

        validSrcAddrs128(mem, plain_ptr, plain_b, plain_len);
        validSrcAddrs128(mem, auth_ptr, auth_b, auth_len);
        validSrcAddrs128(mem, iv_ptr, iv_b, 1);
        validDstAddrs128(mem, out_ptr, out_b, plain_len);
        validDstAddrs128(mem, tag_ptr, tag_b, 1);
        validDstAddrs64(mem, tmp_ptr, tmp_b, 8);

        plain_ptr + 16 * plain_len < pow2_64;
        auth_ptr + 16 * auth_len < pow2_64;
        out_ptr + 16 * plain_len < pow2_64;
        buffer_length(plain_b) == buffer_length(out_b) /\ buffer_length(out_b) == plain_len /\ 256 * buffer_length(plain_b) < pow2_32;
        buffer_length(auth_b) == auth_len;

        iv == buffer128_read(iv_b, 0, mem);

        // To simplify length calculations, restrict auth and plain length further
        256 * auth_len < pow2_32;
        256 * plain_len < pow2_32;

        // AES reqs
        length(round_keys) == 11;
        round_keys == key_to_round_keys(AES_128, reverse_bytes_nat32_seq(key));
        validSrcAddrs128(mem, expanded_key_ptr, keys_b, 11);
        buffer128_as_seq(mem, keys_b) == round_keys;

    ensures
        modifies_mem(loc_union(loc_union(loc_buffer(tmp_b), loc_buffer(out_b)), loc_buffer(tag_b)), old(mem), mem);

        validSrcAddrs128(mem, old(out_ptr), out_b, plain_len);
        validSrcAddrs128(mem, old(tag_ptr), tag_b, 1);

        256 * buffer_length(plain_b) < pow2_32;
        256 * buffer_length(auth_b) < pow2_32;
        buffer128_as_seq(mem, out_b) == fst(gcm_encrypt(AES_128, key, iv, buffer128_as_seq(old(mem), plain_b), buffer128_as_seq(old(mem), auth_b)));
        create(1, buffer128_read(tag_b, 0, mem)) == snd(gcm_encrypt(AES_128, key, iv, mk_gctr_plain(buffer128_as_seq(old(mem), plain_b)), mk_gctr_plain(buffer128_as_seq(old(mem), auth_b))));

        // Calling convention for caller/callee saved registers

        // Windows:
        win ==> rbx == old(rbx);
        win ==> rbp == old(rbp);
        win ==> rdi == old(rdi);
        win ==> rsi == old(rsi);
        win ==> r12 == old(r12);
        win ==> r13 == old(r13);
        win ==> r14 == old(r14);
        win ==> r15 == old(r15);

        // Linux:
        !win ==> rbx == old(rbx);
        !win ==> rbp == old(rbp);
        !win ==> r12 == old(r12);
        !win ==> r13 == old(r13);
        !win ==> r14 == old(r14);
        !win ==> r15 == old(r15);
{
    // Shuffle the incoming pointer around
    inline if (win) {
        Mov64(r9, rcx);
    } else {
        Mov64(r9, rdi);
    }
    // assert r9 == args_ptr;

    Load64_buffer(rax, r9, 64, args_b, 8);
    callee_save_registers(tmp_b);

    // Save a copy of the tmp_ptr
    Mov64(rsi, rax);

    gcm_stdcall_inner(args_b, iv, plain_b, auth_b, iv_b, out_b, tag_b, key, round_keys, keys_b);

    // Restore tmp_ptr
    Mov64(rax, rsi);
    callee_restore_registers(tmp_b);
}
*/
