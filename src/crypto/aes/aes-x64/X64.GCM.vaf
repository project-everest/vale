include "../../../arch/x64/X64.Vale.InsBasic.vaf"
include "../../../arch/x64/X64.Vale.InsMem.vaf"
include "../../../arch/x64/X64.Vale.InsVector.vaf"
include "X64.AES.vaf"
include "X64.GCTR.vaf"
include "X64.GHash.vaf"

module X64.GCM

#verbatim{:interface}{:implementation}
open FStar.Seq
open Types_s
open Types_i
open AES_s
open GCTR_s
open GHash_s
open GHash_i
open GCM_s
open X64.AES
open X64.GHash
open X64.GCTR
open X64.Machine_s
open X64.Vale.State_i
open X64.Vale.Decls_i
open X64.Vale.InsBasic
open X64.Vale.InsMem
open X64.Vale.InsVector
open X64.Vale.InsAes
open X64.Vale.QuickCode_i
open X64.Vale.QuickCodes_i
#endverbatim

#verbatim{:interface}
//let get_last_slice_workaround (s:seq quad32) (start_pos end_pos:int)  =
//  if 0 <= start_pos && start_pos < end_pos && end_pos <= length s then
//    last (slice s start_pos end_pos)
//  else
//    Quad32 0 0 0 0 
//
//let slice_workaround (s:seq quad32) (start_pos end_pos:int)  =
//  if 0 <= start_pos && start_pos < end_pos && end_pos <= length s then
//    slice s start_pos end_pos
//  else
//    create 1 (Quad32 0 0 0 0)
#endverbatim

#reset-options "--z3rlimit 40"

///////////////////////////
// GCM
///////////////////////////

procedure {:quick exportOnly} gcm_core(
    ghost plain_b:buffer128,
    ghost auth_b:buffer128,
    ghost out_b:buffer128,
    ghost key:aes_key(AES_128),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )

    lets plain_ptr @= r12; out_ptr @= rbx; keys_ptr @= r8; auth_ptr @= r14;
    plain_len @= r13; auth_len @= r11; 
    iv @= xmm3;

    reads
        plain_ptr; out_ptr; keys_ptr; auth_ptr; plain_len; auth_len; 

    modifies
        rax; rcx; rdx; r9; r10; xmm0; xmm1; xmm2; xmm4; xmm5; iv;
        mem; efl; 

    requires
        // GCM reqs
        buffers_disjoint128(plain_b, out_b);
        buffers_disjoint128(auth_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        validSrcAddrs128(mem, plain_ptr, plain_b, plain_len);
        validSrcAddrs128(mem, auth_ptr, auth_b, auth_len);
        validDstAddrs128(mem, out_ptr, out_b, plain_len);
        plain_ptr + 16 * plain_len < nat64_max;
        auth_ptr + 16 * auth_len < nat64_max;
        out_ptr + 16 * plain_len < nat64_max;
        buffer_length(plain_b) == buffer_length(out_b) /\ buffer_length(out_b) == plain_len /\ 256 * buffer_length(plain_b) < nat32_max;
        buffer_length(auth_b) == auth_len;

        // To simplify length calculations, restrict auth and plain length further
        256 * auth_len < nat32_max;
        256 * plain_len < nat32_max;

        // AES reqs
        length(round_keys) == 11;
        round_keys == key_to_round_keys(AES_128, key);
        validSrcAddrs128(mem, keys_ptr, keys_b, 11);
        buffer128_as_seq(mem, keys_b) == round_keys;
    ensures
        modifies_buffer128(out_b, old(mem), mem);
        validSrcAddrs128(mem, out_ptr, out_b, plain_len);
        256 * buffer_length(plain_b) < nat32_max;
        256 * buffer_length(auth_b) < nat32_max;
        buffer128_as_seq(mem, out_b) == fst(gcm_encrypt(AES_128, key, iv, buffer128_as_seq(old(mem), plain_b), buffer128_as_seq(old(mem), auth_b)));
        create(1, xmm1) == snd(gcm_encrypt(AES_128, key, iv, buffer128_as_seq(old(mem), plain_b), buffer128_as_seq(old(mem), auth_b)));
{
    // let h = aes_encrypt alg key (Quad32 0 0 0 0) in
    Pxor(xmm0, xmm0);
    lemma_quad32_xor(); // xmm0 = 0
    AES128EncryptBlock(xmm0, key, round_keys, keys_b); // h = xmm0 = aes_encrypt alg key (Quad32 0 0 0 0) in
    ghost var h := xmm0;
    Mov128(xmm5, xmm0);     // Save a copy of h

    // let j0 = Quad32 1 iv.mid_lo iv.mid_hi iv.hi in
    // (inc32 j0 1)
    Pinsrd(iv, 2, 0);
    // assert iv == inc32(old(Quad32(1,iv.mid_lo,iv.mid_hi,iv.hi)), 1);

    // let c = gctr_encrypt (inc32 j0 1) p alg key in
    Mov64(rax, plain_ptr);
    Mov64(rcx, plain_len);
    gctr_core(plain_b, out_b, key, round_keys, keys_b);

    // Compute the hashes incrementally
    Mov64(rax, auth_ptr);
    Mov64(rcx, auth_len);
    Mov128(xmm2, xmm5);     // Move h into xmm2, where ghash expects it
    assert xmm2 == h;
    ghost var y_0 := Quad32(0, 0, 0, 0);
    compute_Y0();
    assert xmm0 == y_0;
    compute_ghash_incremental(auth_b);
    ghost var y_auth := xmm0;

    Mov64(rax, out_ptr);
    Mov64(rcx, plain_len);

    compute_ghash_incremental(out_b);
    ghost var y_cipher := xmm0;

    // Prepare length fields
    Pxor(xmm1, xmm1);
    Pinsrd(xmm1, auth_len, 1);
    Pinsrd(xmm1, plain_len, 3);

    ghost var length_quad32 := xmm1;

    compute_ghash_incremental_register();
    ghost var y_final := xmm0;

    // Invoke lemma showing that incremental hashing works
    lemma_hash_append3(h, y_0, y_auth, y_cipher, y_final,
                       buffer128_as_seq(mem, auth_b),
                       buffer128_as_seq(mem, out_b),
                       create(1, length_quad32)); 

    Pinsrd(iv, 1, 0);   // Reconstruct j0 (this is all we need, since gctr_core says it only changes iv.lo)

    // Put the hash value currently in xmm0 into xmm1, where gctr_register will encrypt it 
    Mov128(xmm1, xmm0);
    gctr_register(key, round_keys, keys_b); // Encrypt using j0 and xmm0 = hash_value 
}

  
