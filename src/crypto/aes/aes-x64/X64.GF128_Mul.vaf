include "../../../arch/x64/X64.Vale.InsBasic.vaf"
include "../../../arch/x64/X64.Vale.InsVector.vaf"
include "../../../arch/x64/X64.Vale.InsAes.vaf"

module X64.GF128_Mul

#verbatim{:interface}{:implementation}
open Types_s
open Types_i
open Math.Poly2_s
open Math.Poly2_i
open Math.Poly2.Bits_s
open Math.Poly2.Bits_i
open Math.Poly2.Lemmas_i
open GF128_s
open GF128_i
open X64.Machine_s
open X64.Vale.State_i
open X64.Vale.Decls_i
open X64.Vale.InsBasic
open X64.Vale.InsMem
open X64.Vale.InsVector
open X64.Vale.InsAes
open X64.Vale.QuickCode_i
open X64.Vale.QuickCodes_i
#endverbatim

procedure{:quick} ShiftLeft128_1(ghost a:poly)
    modifies
        efl;
        xmm1; xmm2;
    requires
        degree(a) < 127;
        xmm1 == to_quad32(a);
    ensures
        xmm1 == to_quad32(shift(a, 1));
{
    Mov128(xmm2, xmm1);
    Psrld(xmm2, 31);
    Pslld(xmm1, 1);
    VPSLLDQ4(xmm2, xmm2);
    Pxor(xmm1, xmm2);

    lemma_shift_left_1(a);
}

procedure{:quick} ShiftLeft2_128_1(ghost lo:poly, ghost hi:poly)
    modifies
        efl;
        xmm1; xmm2; xmm3; xmm4; xmm5;
    requires
        degree(hi) < 127;
        degree(lo) <= 127;
        xmm1 == to_quad32(lo);
        xmm2 == to_quad32(hi);
    ensures
        let n := monomial(128);
        let a := add(mul(hi, n), lo);
        let b := shift(a, 1);
        xmm1 == to_quad32(mod(b, n));
        xmm2 == to_quad32(div(b, n));
{
    // qa := xmm1
    // qb := xmm2

    // ra := map(>> 31)(qa)
    Mov128(xmm3, xmm1);
    Psrld(xmm3, 31);

    // rb := map(>> 31)(qb)
    Mov128(xmm4, xmm2);
    Psrld(xmm4, 31);

    // la := map(<< 1)(qa)
    // lb := map(<< 1)(qb)
    Pslld(xmm1, 1);
    Pslld(xmm2, 1);

    // ra012 := ra << 32
    // rb012 := rb << 32
    VPSLLDQ4(xmm5, xmm3);
    VPSLLDQ4(xmm4, xmm4);

    // ra3 := ra >> 96
    Pinsrd(xmm3, 0, 0);
    Pshufd(xmm3, xmm3, 0x03); // (hi) 0 0 0 3 (lo)

    // ra3_rb012 := ra3 + rb012
    Pxor(xmm3, xmm4);

    // qa' := la + ra012
    // qb' := lb + ra3_rb012
    Pxor(xmm1, xmm5);
    Pxor(xmm2, xmm3);

    lemma_shift_2_left_1(lo, hi);
}

procedure{:quick} ClmulRev64(ghost a:poly, ghost b:poly, inline dstHi:bool, inline srcHi:bool)
    reads
    modifies
        efl;
        xmm1; xmm2;
    requires
        degree(a) <= 63;
        degree(b) <= 63;
        reverse(a, 63) == of_double32(if dstHi then quad32_double_hi(xmm1) else quad32_double_lo(xmm1));
        reverse(b, 63) == of_double32(if srcHi then quad32_double_hi(xmm2) else quad32_double_lo(xmm2));
    ensures
        xmm1 == to_quad32(reverse(mul(a, b), 127));
{
    Pclmulqdq(xmm1, xmm2, dstHi, srcHi);
    ShiftLeft128_1(mul(reverse(a, 63), reverse(b, 63)));

    lemma_mul_reverse_shift_1(a, b, 63);
}

procedure{:quick exportOnly} High64ToLow(out dst:xmm, ghost a:poly)
    modifies
        efl;
    requires
        degree(a) <= 127;
        dst == to_quad32(a);
    ensures
        dst == to_quad32(div(a, monomial(64)));
{
    Pinsrd(dst, 0, 0);
    Pshufd(dst, dst, 0x0e); // (hi) 0 0 3 2 (lo)
    lemma_quad32_double_shift(a);
}

procedure{:quick exportOnly} Low64ToHigh(out dst:xmm, ghost a:poly)
    modifies
        efl;
    requires
        degree(a) <= 127;
        dst == to_quad32(a);
    ensures
        dst == to_quad32(mul(mod(a, monomial(64)), monomial(64)));
{
    Pinsrd(dst, 0, 3);
    Pshufd(dst, dst, 0x4f); // (hi) 1 0 3 3 (lo)
    lemma_quad32_double_shift(a);
}

procedure{:quick exportOnly} AddPoly(out dst:xmm, src:xmm, ghost a:poly, ghost b:poly)
    modifies
        efl;
    requires
        degree(a) <= 127;
        degree(b) <= 127;
        dst == to_quad32(a);
        src == to_quad32(b);
    ensures
        dst == to_quad32(add(a, b));
{
    Pxor(dst, src);
    lemma_add128(a, b);
}

procedure{:quick} Clmul128(ghost ab:poly, ghost cd:poly) returns(ghost lo:poly, ghost hi:poly)
    modifies
        efl;
        xmm1; xmm2; xmm3; xmm4; xmm5;
    requires
        degree(ab) <= 127;
        degree(cd) <= 127;
        xmm1 == to_quad32(ab);
        xmm2 == to_quad32(cd);
    ensures
        degree(lo) <= 127;
        degree(hi) < 127;
        mul(ab, cd) == add(shift(hi, 128), lo);
        xmm1 == to_quad32(lo);
        xmm2 == to_quad32(hi);
{
    let n := monomial(64);
    let a := div(ab, n);
    let b := mod(ab, n);
    let c := div(cd, n);
    let d := mod(cd, n);
    let ac := mul(a, c);
    let ad := mul(a, d);
    let bc := mul(b, c);
    let bd := mul(b, d);
    lemma_div_mod(ab, n);
    lemma_quad32_double(ab);
    lemma_quad32_double(cd);

    Mov128(xmm5, xmm1);

    // xmm3 := bc
    Pclmulqdq(xmm1, xmm2, false, true);
    Mov128(xmm3, xmm1);

    // xmm4 := ad
    Mov128(xmm1, xmm5);
    Pclmulqdq(xmm1, xmm2, true, false);
    Mov128(xmm4, xmm1);

    // xmm1 := bd
    Mov128(xmm1, xmm5);
    Pclmulqdq(xmm1, xmm2, false, false);

    // xmm5 := ac
    Pclmulqdq(xmm5, xmm2, true, true);

    // xmm2 := ac + hi(bc) + hi(ad)
    Mov128(xmm2, xmm5); // ac
    Mov128(xmm5, xmm1); // bd
    Mov128(xmm1, xmm3); // bc
    High64ToLow(xmm1, bc);
    AddPoly(xmm2, xmm1, ac, div(bc, n));
    Mov128(xmm1, xmm4); // ad
    High64ToLow(xmm1, ad);
    AddPoly(xmm2, xmm1, add(ac, div(bc, n)), div(ad, n));

    // xmm1 := lo(bc) * n + lo(ad) * n + bd
    Mov128(xmm1, xmm3); // bc
    Low64ToHigh(xmm1, bc);
    Low64ToHigh(xmm4, ad);
    AddPoly(xmm1, xmm4, mul(mod(bc, n), n), mul(mod(ad, n), n));
    AddPoly(xmm1, xmm5, add(mul(mod(bc, n), n), mul(mod(ad, n), n)), bd);

    hi := add(add(ac, div(bc, n)), div(ad, n));
    lo := add(add(mul(mod(bc, n), n), mul(mod(ad, n), n)), bd);
    lemma_gf128_mul(a, b, c, d, 64);
}

procedure{:quick} ClmulRev128(ghost ab:poly, ghost cd:poly) returns(ghost lo:poly, ghost hi:poly)
    modifies
        efl;
        xmm1; xmm2; xmm3; xmm4; xmm5;
    requires
        degree(ab) <= 127;
        degree(cd) <= 127;
        xmm1 == to_quad32(reverse(ab, 127));
        xmm2 == to_quad32(reverse(cd, 127));
    ensures
        degree(lo) <= 127;
        degree(hi) <= 127;
        reverse(mul(ab, cd), 255) == add(shift(hi, 128), lo);
        xmm1 == to_quad32(lo);
        xmm2 == to_quad32(hi);
{
    lo, hi := Clmul128(reverse(ab, 127), reverse(cd, 127));
    ShiftLeft2_128_1(lo, hi);

    let m := shift(add(shift(hi, 128), lo), 1);
    lemma_combine_define(hi, lo, 128);
    lemma_split_define(m, 128);
    lo := mod(m, monomial(128));
    hi := div(m, monomial(128));
    lemma_mul_reverse_shift_1(ab, cd, 127);
}

procedure{:quick exportOnly} Gf128ModulusRev(inout dst:xmm)
    modifies
        efl;
    ensures
        dst == to_quad32(reverse(gf128_modulus_low_terms, 127));
{
    lemma_gf128_constant_rev(dst);
    Pxor(dst, dst);
    Pinsrd(dst, 0xe100_0000, 3);
}

procedure{:quick} ReduceMulRev128(ghost a:poly, ghost b:poly)
    modifies
        efl;
        xmm1; xmm2; xmm3; xmm4; xmm5; xmm6;
    requires
        degree(a) <= 127;
        degree(b) <= 127;
        xmm1 == to_quad32(reverse(a, 127));
        xmm2 == to_quad32(reverse(b, 127));
    ensures
        xmm1 == to_quad32(reverse(gf128_mul(a, b), 127));
{
    lemma_gf128_degree();
    lemma_gf128_reduce_rev(a, b, gf128_modulus_low_terms, 128);
    let m := monomial(128);
    let h := gf128_modulus_low_terms;
    let ab := mul(a, b);
    let rh := reverse(h, 127);
    let rab := reverse(ab, 255);
    let rd := mod(rab, m);
    let rdh := reverse(mul(reverse(rd, 127), h), 255);
    let rdhL := mod(rdh, m);
    let rdhLh := reverse(mul(reverse(rdhL, 127), h), 127);

    (ghost var lo1), (ghost var hi1) := ClmulRev128(a, b);
    lemma_combine_define(hi1, lo1, 128);
    Mov128(xmm6, xmm2); // div(rab, m);

    Gf128ModulusRev(xmm2);
    // REVIEW: h is small, so we could make a more efficient version of ClmulRev128 for this:
    (ghost var lo2), (ghost var hi2) := ClmulRev128(reverse(rd, 127), h);
    lemma_combine_define(hi2, lo2, 128);
    Mov128(xmm5, xmm2); // div(rdh, m);

    Gf128ModulusRev(xmm2);

    lemma_quad32_double_hi_rev(rdhL);
    lemma_quad32_double_hi_rev(rh);
    ClmulRev64(reverse(rdhL, 127), h, true, true);

    AddPoly(xmm1, xmm5, rdhLh, div(rdh, m));
    AddPoly(xmm1, xmm6, add(rdhLh, div(rdh, m)), div(rab, m));
}
